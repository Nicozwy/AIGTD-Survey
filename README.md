# AIGTD：Recent Advances in AI-generated Text Detection
The classification topology of AIGTD is constructed based on addressing three key challenges: classifier training, inherent attributes, and information embedding.

`Table of Contents`
- [Tackling Classifier Training](#tackling-classifier-training)
- [Tackling Intrinsic Attributes](#tackling-intrinsic-attributes)
- [Tackling Information Embedding](#tackling-information-embedding)
- [Citation](#citation)
  
>  @FZJ负责协调、进度把控；


## Tackling Classifier Training
> @ZHH



## Tackling Intrinsic Attributes
> @NRC


## Tackling Information Embedding
1. Mercan Topkara, Cuneyt M Taskiran, and Edward J Delp III. Natural language watermarking. In Security, Steganography, and Watermarking of Multimedia Contents VII, volume 5681, pages 441–452. SPIE, 2005. [[paper](https://www.cerias.purdue.edu/tools_and_resources/bibtex_archive/archive/PSI000441.pdf)]

2. Umut Topkara, Mercan Topkara, and Mikhail J Atallah. The hiding virtues of ambiguity: quantifiably resilient watermarking of natural language text through synonym substitutions. In Proceedings of the 8th workshop on Multimedia and security, pages 164–174, 2006. [[paper](http://umut.topkara.org/papers/ToToAt_MMSEC06.pdf)]

3. Xi Yang, Jie Zhang, Kejiang Chen, Weiming Zhang, Zehua Ma, Feng Wang, and Nenghai Yu. Tracing text provenance via context-aware lexical substitution. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 36, pages 11613–11621, 2022. [[paper](https://cdn.aaai.org/ojs/21415/21415-13-25428-1-2-20220628.pdf)]

4. Xi Yang, Kejiang Chen, Weiming Zhang, Chang Liu, Yuang Qi, Jie Zhang, Han Fang, and Nenghai Yu. Watermarking text generated by black-box language models. arXiv preprint arXiv:2305.08883, 2023. [[paper](https://arxiv.org/pdf/2305.08883)]

5. Wenjie Qu, Dong Yin, Zixin He, Wei Zou, Tianyang Tao, Jinyuan Jia, and Jiaheng Zhang. Provably robust multi-bit watermarking for ai-generated text via error correction code. arXiv preprint arXiv:2401.16820, 2024. [[paper](https://arxiv.org/pdf/2401.16820)]

### Training-free

#### Logits Deviation
1. John Kirchenbauer, Jonas Geiping, Yuxin Wen, Jonathan Katz, Ian Miers, and Tom Goldstein. A watermark for large language models. In International Conference on Machine Learning, pages 17061–17084. PMLR, 2023. [[paper](https://proceedings.mlr.press/v202/kirchenbauer23a/kirchenbauer23a.pdf)]

2. Xuandong Zhao, Prabhanjan Ananth, Lei Li, and Yu-Xiang Wang. Provable robust watermarking for ai-generated text. arXiv preprint arXiv:2306.17439, 2023. [[paper](https://arxiv.org/pdf/2306.17439)]

#### Hash-based
1. Abe Bohan Hou, Jingyu Zhang, Tianxing He, Yichen Wang, YungSung Chuang, Hongwei Wang, Lingfeng Shen, Benjamin Van Durme, Daniel Khashabi, and Yulia Tsvetkov. Semstamp: A semantic watermark with paraphrastic robustness for text generation. arXiv preprint arXiv:2310.03991, 2023. [[paper](https://arxiv.org/pdf/2310.03991)]

2. Yihan Wu, Zhengmian Hu, Hongyang Zhang, and Heng Huang. Dipmark: A stealthy, efficient and resilient watermark for large language models. arXiv preprint arXiv:2310.07710, 2023. [[paper](https://arxiv.org/pdf/2310.07710)]

3. Abe Bohan Hou, Jingyu Zhang, Yichen Wang, Daniel Khashabi, and Tianxing He. k-semstamp: A clustering-based semantic watermark for detection of machine-generated text. arXiv preprint arXiv:2402.11399, 2024. [[paper](https://arxiv.org/pdf/2402.11399)]

#### Message Decoding
1. Xuandong Zhao, Lei Li, and Yu-Xiang Wang. Permute-and-flip: An optimally robust and watermarkable decoder for llms. arXiv preprint arXiv:2402.05864, 2024. [[paper](https://arxiv.org/pdf/2402.05864)]

2. Scott Aaronson, Jiahui Liu, Qipeng Liu, Mark Zhandry, and Ruizhe Zhang. New approaches for quantum copy-protection. In Advances in Cryptology–CRYPTO 2021: 41st Annual International Cryptology Conference, CRYPTO 2021, Virtual Event, August 16–20, 2021, Proceedings, Part I 41, pages 526–555. Springer, 2021. [[paper](https://arxiv.org/pdf/2004.09674)]

###  Training-based
#### Message Encoding
1. Han Fang, Zhaoyang Jia, Hang Zhou, Zehua Ma, and Weiming Zhang. Encoded feature enhancement in watermarking network for distortion in real scenes. IEEE Transactions on Multimedia, 2022. [[paper](http://staff.ustc.edu.cn/~zhangwm/Paper/2022_19.pdf)]

2. Ruisi Zhang, Shehzeen Samarah Hussain, Paarth Neekhara, and Farinaz Koushanfar. Remark-llm: A robust and efficient watermarking framework for generative large language models. arXiv preprint arXiv:2310.12362, 2023. [[paper](https://arxiv.org/pdf/2310.12362)]

### Information Capacity

#### Multi-bit
1. KiYoon Yoo, Wonhyuk Ahn, Jiho Jang, and Nojun Kwak. Robust multi-bit natural language watermarking through invariant features. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 2092–2115, 2023. [[paper](https://arxiv.org/pdf/2305.01904)]

2. Pierre Fernandez, Antoine Chaffin, Karim Tit, Vivien Chappelier, and Teddy Furon. Three bricks to consolidate watermarks for large language models. In 2023 IEEE International Workshop on Information Forensics and Security (WIFS), pages 1–6. IEEE, 2023. [[paper](https://arxiv.org/pdf/2308.00113)]

3. Massieh Kordi Boroujeny, Ya Jiang, Kai Zeng, and Brian Mark. Multi-bit distortion-free watermarking for large language models. arXiv preprint arXiv:2402.16578, 2024. [[paper](https://arxiv.org/pdf/2402.16578)]


## Citation
If you find this project useful in your research or work, please consider citing it:

```
Coming soon...
```


## Acknowledgements
Your contributions will be acknowledged. 

[Github Flavored Markdown](https://github.com/guodongxiaren/README)
